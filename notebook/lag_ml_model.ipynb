{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Close', 'High', 'Low', 'Open', 'Volume', 'Direction',\n",
      "       'sentiment_score', 'shifted_direction'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('/home/sacsresta/Documents/RESEARCH/Project/sentiment/merged_data_AAPL_from_2015-01-01_to_2025-03-01.csv')\n",
    "df['shifted_direction'] = df['Direction'].shift(-1)\n",
    "df = df.drop(columns=['Supertrend','UpperBand', 'LowerBand', 'Uptrend',\n",
    "       'Downtrend', 'headline','Adj Close'])\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.dropna(inplace=True)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X = df[['Date', 'Close', 'High', 'Low', 'Open', 'Volume', \n",
    "       'sentiment_score', 'shifted_direction']].copy()\n",
    "def create_data(X, max_lag = 60):\n",
    "  # Define the number of lags you want\n",
    "  max_lag = max_lag  # For example, to create lags 1 through 5\n",
    "\n",
    "  # Create lagged features in a loop\n",
    "  for i in range(1, max_lag + 1):\n",
    "      X[f'Close_lag{i}'] = X['Close'].shift(i)\n",
    "\n",
    "  # Define X and y (drop the first two rows with NaN lags)\n",
    "  y = X['shifted_direction'].iloc[max_lag:].astype(int)\n",
    "  X = X.drop(columns = ['shifted_direction']).iloc[max_lag:]\n",
    "  return X,y\n",
    "\n",
    "X,y = create_data(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2494, 67), (2494,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date           0\n",
       "Close          0\n",
       "High           0\n",
       "Low            0\n",
       "Open           0\n",
       "              ..\n",
       "Close_lag56    0\n",
       "Close_lag57    0\n",
       "Close_lag58    0\n",
       "Close_lag59    0\n",
       "Close_lag60    0\n",
       "Length: 67, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shifted_direction\n",
       " 1    1492\n",
       "-1    1002\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.set_index('Date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1745, 66), (749, 66), (1745,), (749,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle = False)\n",
    "\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report,confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Performance:\n",
      "Accuracy: 0.8879\n",
      "Confusion Matrix:\n",
      "[[310  45]\n",
      " [ 39 355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88       355\n",
      "           1       0.89      0.90      0.89       394\n",
      "\n",
      "    accuracy                           0.89       749\n",
      "   macro avg       0.89      0.89      0.89       749\n",
      "weighted avg       0.89      0.89      0.89       749\n",
      "\n",
      "\n",
      "Random Forest Performance:\n",
      "Accuracy: 0.6275\n",
      "Confusion Matrix:\n",
      "[[229 126]\n",
      " [153 241]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.65      0.62       355\n",
      "           1       0.66      0.61      0.63       394\n",
      "\n",
      "    accuracy                           0.63       749\n",
      "   macro avg       0.63      0.63      0.63       749\n",
      "weighted avg       0.63      0.63      0.63       749\n",
      "\n",
      "\n",
      "XGBoost Performance:\n",
      "Accuracy: 0.5394\n",
      "Confusion Matrix:\n",
      "[[ 10 345]\n",
      " [  0 394]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.03      0.05       355\n",
      "           1       0.53      1.00      0.70       394\n",
      "\n",
      "    accuracy                           0.54       749\n",
      "   macro avg       0.77      0.51      0.38       749\n",
      "weighted avg       0.75      0.54      0.39       749\n",
      "\n",
      "\n",
      "SVM Performance:\n",
      "Accuracy: 0.8865\n",
      "Confusion Matrix:\n",
      "[[319  36]\n",
      " [ 49 345]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.90      0.88       355\n",
      "           1       0.91      0.88      0.89       394\n",
      "\n",
      "    accuracy                           0.89       749\n",
      "   macro avg       0.89      0.89      0.89       749\n",
      "weighted avg       0.89      0.89      0.89       749\n",
      "\n",
      "\n",
      "Naive Bayes Performance:\n",
      "Accuracy: 0.5260\n",
      "Confusion Matrix:\n",
      "[[  0 355]\n",
      " [  0 394]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       355\n",
      "           1       0.53      1.00      0.69       394\n",
      "\n",
      "    accuracy                           0.53       749\n",
      "   macro avg       0.26      0.50      0.34       749\n",
      "weighted avg       0.28      0.53      0.36       749\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Boosting Performance:\n",
      "Accuracy: 0.5781\n",
      "Confusion Matrix:\n",
      "[[251 104]\n",
      " [212 182]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.71      0.61       355\n",
      "           1       0.64      0.46      0.54       394\n",
      "\n",
      "    accuracy                           0.58       749\n",
      "   macro avg       0.59      0.58      0.57       749\n",
      "weighted avg       0.59      0.58      0.57       749\n",
      "\n",
      "\n",
      "K-Nearest Neighbors Performance:\n",
      "Accuracy: 0.6649\n",
      "Confusion Matrix:\n",
      "[[217 138]\n",
      " [113 281]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.61      0.63       355\n",
      "           1       0.67      0.71      0.69       394\n",
      "\n",
      "    accuracy                           0.66       749\n",
      "   macro avg       0.66      0.66      0.66       749\n",
      "weighted avg       0.66      0.66      0.66       749\n",
      "\n",
      "\n",
      "Decision Tree Performance:\n",
      "Accuracy: 0.5794\n",
      "Confusion Matrix:\n",
      "[[ 59 296]\n",
      " [ 19 375]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.17      0.27       355\n",
      "           1       0.56      0.95      0.70       394\n",
      "\n",
      "    accuracy                           0.58       749\n",
      "   macro avg       0.66      0.56      0.49       749\n",
      "weighted avg       0.65      0.58      0.50       749\n",
      "\n",
      "\n",
      "AdaBoost Performance:\n",
      "Accuracy: 0.5274\n",
      "Confusion Matrix:\n",
      "[[  2 353]\n",
      " [  1 393]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.01      0.01       355\n",
      "           1       0.53      1.00      0.69       394\n",
      "\n",
      "    accuracy                           0.53       749\n",
      "   macro avg       0.60      0.50      0.35       749\n",
      "weighted avg       0.59      0.53      0.37       749\n",
      "\n",
      "\n",
      "SGD Classifier Performance:\n",
      "Accuracy: 0.8838\n",
      "Confusion Matrix:\n",
      "[[330  25]\n",
      " [ 62 332]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.88       355\n",
      "           1       0.93      0.84      0.88       394\n",
      "\n",
      "    accuracy                           0.88       749\n",
      "   macro avg       0.89      0.89      0.88       749\n",
      "weighted avg       0.89      0.88      0.88       749\n",
      "\n",
      "\n",
      "MLP Performance:\n",
      "Accuracy: 0.8478\n",
      "Confusion Matrix:\n",
      "[[272  83]\n",
      " [ 31 363]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.77      0.83       355\n",
      "           1       0.81      0.92      0.86       394\n",
      "\n",
      "    accuracy                           0.85       749\n",
      "   macro avg       0.86      0.84      0.85       749\n",
      "weighted avg       0.85      0.85      0.85       749\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 1098, number of negative: 647\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000962 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16830\n",
      "[LightGBM] [Info] Number of data points in the train set: 1745, number of used features: 66\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.629226 -> initscore=0.528899\n",
      "[LightGBM] [Info] Start training from score 0.528899\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "LGBMClassifier Performance:\n",
      "Accuracy: 0.6368\n",
      "Confusion Matrix:\n",
      "[[ 87 268]\n",
      " [  4 390]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.25      0.39       355\n",
      "           1       0.59      0.99      0.74       394\n",
      "\n",
      "    accuracy                           0.64       749\n",
      "   macro avg       0.77      0.62      0.57       749\n",
      "weighted avg       0.76      0.64      0.57       749\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "# Updated classifier dictionary\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(C=10, max_iter=1000, solver='liblinear'),\n",
    "    'Random Forest': RandomForestClassifier(max_depth=10, min_samples_split=10, n_estimators=50),\n",
    "    'XGBoost': XGBClassifier(learning_rate=0.01, max_depth=3, n_estimators=100, eval_metric='logloss'),\n",
    "    'SVM': SVC(C=10, kernel='linear', probability=True),\n",
    "    'Naive Bayes': BernoulliNB(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3),  # Uses decision trees [[7]]\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),  # Instance-based learning\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, min_samples_split=10),  # Base tree model [[7]]\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=50, learning_rate=0.5),  # Boosting ensemble\n",
    "    'SGD Classifier': SGDClassifier(loss='log_loss', alpha=0.001),  # Linear model with SGD [[10]]\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000),  # Neural network\n",
    "    'LGBMClassifier' :LGBMClassifier(\n",
    "    n_estimators=100, \n",
    "    learning_rate=0.05, \n",
    "    max_depth=5, \n",
    "    boosting_type='gbdt'\n",
    ")\n",
    "}\n",
    "# Create list to store results\n",
    "results = []\n",
    "\n",
    "# Train-test loop\n",
    "for name, clf in classifiers.items():\n",
    "    # Train model\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results.append((name, accuracy, cm))\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\n{name} Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=10, max_iter=1000, solver='liblinear')\n",
      "RandomForestClassifier(max_depth=10, min_samples_split=10, n_estimators=50)\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='logloss',\n",
      "              feature_types=None, feature_weights=None, gamma=None,\n",
      "              grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)\n",
      "SVC(C=10, kernel='linear', probability=True)\n",
      "BernoulliNB()\n",
      "GradientBoostingClassifier()\n",
      "KNeighborsClassifier()\n",
      "DecisionTreeClassifier(max_depth=5, min_samples_split=10)\n",
      "AdaBoostClassifier(learning_rate=0.5)\n",
      "SGDClassifier(alpha=0.001, loss='log_loss')\n",
      "MLPClassifier(max_iter=1000)\n",
      "LGBMClassifier(learning_rate=0.05, max_depth=5)\n"
     ]
    }
   ],
   "source": [
    "for i, k in classifiers.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing Logistic Regression...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression: {'C': 10, 'solver': 'liblinear'}\n",
      "Optimizing Random Forest...\n",
      "Best parameters for Random Forest: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Optimizing XGBoost...\n",
      "Best parameters for XGBoost: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}\n",
      "Optimizing SVM...\n",
      "Best parameters for SVM: {'C': 10, 'kernel': 'linear'}\n",
      "Optimizing Naive Bayes...\n",
      "Best parameters for Naive Bayes: {}\n",
      "Optimizing Gradient Boosting...\n",
      "Optimizing K-Nearest Neighbors...\n",
      "Optimizing Decision Tree...\n",
      "Optimizing AdaBoost...\n",
      "Optimizing SGD Classifier...\n",
      "Optimizing MLP...\n",
      "Optimizing LGBMClassifier...\n",
      "[LightGBM] [Info] Number of positive: 1098, number of negative: 647\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16830\n",
      "[LightGBM] [Info] Number of data points in the train set: 1745, number of used features: 66\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.629226 -> initscore=0.528899\n",
      "[LightGBM] [Info] Start training from score 0.528899\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "No hyperparameters to tune for LGBMClassifier\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameter grids\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.1, 1, 10], \n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [5, 10, None],\n",
    "        'min_samples_split': [2, 10],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 5],\n",
    "        'n_estimators': [100, 200],\n",
    "        'subsample': [0.6, 0.8],\n",
    "        'colsample_bytree': [0.6, 1.0]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'Naive Bayes': {\n",
    "        'alpha': [0.1, 0.5, 1.0]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'max_depth': [3, 5],\n",
    "        'min_samples_split': [2, 10]\n",
    "    },\n",
    "    'K-Nearest Neighbors': {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree'],\n",
    "        'p': [1, 2]\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [5, 10, None],\n",
    "        'min_samples_split': [2, 10],\n",
    "        'max_features': [None, 'sqrt']\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'n_estimators': [50, 100],\n",
    "        'learning_rate': [0.1, 0.5, 1.0]\n",
    "    },\n",
    "    'SGD Classifier': {\n",
    "        'alpha': [0.0001, 0.001],\n",
    "        'loss': ['log_loss', 'modified_huber'],\n",
    "        'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "        'learning_rate': ['constant', 'optimal']\n",
    "    },\n",
    "    'MLP': {\n",
    "        'hidden_layer_sizes': [(100,), (50,50)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'solver': ['adam', 'sgd'],\n",
    "        'learning_rate_init': [0.001, 0.01],\n",
    "        'alpha': [0.0001, 0.001]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Dictionary to store best models\n",
    "best_models = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Optimizing {name}...\")\n",
    "    \n",
    "    if name in param_grids:\n",
    "        grid_search = GridSearchCV(clf, param_grids[name], cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        best_models[name] = grid_search.best_estimator_  # Save best model\n",
    "        print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "    else:\n",
    "        clf.fit(X_train_scaled, y_train)\n",
    "        best_models[name] = clf\n",
    "print(f\"No hyperparameters to tune for {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': LogisticRegression(C=10, max_iter=1000, solver='liblinear'),\n",
       " 'Random Forest': RandomForestClassifier(max_depth=10, n_estimators=50),\n",
       " 'XGBoost': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "               colsample_bylevel=None, colsample_bynode=None,\n",
       "               colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "               enable_categorical=False, eval_metric='logloss',\n",
       "               feature_types=None, feature_weights=None, gamma=None,\n",
       "               grow_policy=None, importance_type=None,\n",
       "               interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "               max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "               max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "               min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "               multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "               num_parallel_tree=None, ...),\n",
       " 'SVM': SVC(C=10, kernel='linear', probability=True),\n",
       " 'Naive Bayes': BernoulliNB(),\n",
       " 'Gradient Boosting': GradientBoostingClassifier(),\n",
       " 'K-Nearest Neighbors': KNeighborsClassifier(),\n",
       " 'Decision Tree': DecisionTreeClassifier(max_depth=5, min_samples_split=10),\n",
       " 'AdaBoost': AdaBoostClassifier(learning_rate=0.5),\n",
       " 'SGD Classifier': SGDClassifier(alpha=0.001, loss='log_loss'),\n",
       " 'MLP': MLPClassifier(max_iter=1000),\n",
       " 'LGBMClassifier': LGBMClassifier(learning_rate=0.05, max_depth=5)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Performance:\n",
      "Accuracy: 0.8879\n",
      "Confusion Matrix:\n",
      "[[310  45]\n",
      " [ 39 355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88       355\n",
      "           1       0.89      0.90      0.89       394\n",
      "\n",
      "    accuracy                           0.89       749\n",
      "   macro avg       0.89      0.89      0.89       749\n",
      "weighted avg       0.89      0.89      0.89       749\n",
      "\n",
      "\n",
      "Random Forest Performance:\n",
      "Accuracy: 0.6088\n",
      "Confusion Matrix:\n",
      "[[255 100]\n",
      " [193 201]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.72      0.64       355\n",
      "           1       0.67      0.51      0.58       394\n",
      "\n",
      "    accuracy                           0.61       749\n",
      "   macro avg       0.62      0.61      0.61       749\n",
      "weighted avg       0.62      0.61      0.61       749\n",
      "\n",
      "\n",
      "XGBoost Performance:\n",
      "Accuracy: 0.5394\n",
      "Confusion Matrix:\n",
      "[[ 10 345]\n",
      " [  0 394]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.03      0.05       355\n",
      "           1       0.53      1.00      0.70       394\n",
      "\n",
      "    accuracy                           0.54       749\n",
      "   macro avg       0.77      0.51      0.38       749\n",
      "weighted avg       0.75      0.54      0.39       749\n",
      "\n",
      "\n",
      "SVM Performance:\n",
      "Accuracy: 0.8865\n",
      "Confusion Matrix:\n",
      "[[319  36]\n",
      " [ 49 345]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.90      0.88       355\n",
      "           1       0.91      0.88      0.89       394\n",
      "\n",
      "    accuracy                           0.89       749\n",
      "   macro avg       0.89      0.89      0.89       749\n",
      "weighted avg       0.89      0.89      0.89       749\n",
      "\n",
      "\n",
      "Naive Bayes Performance:\n",
      "Accuracy: 0.5260\n",
      "Confusion Matrix:\n",
      "[[  0 355]\n",
      " [  0 394]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       355\n",
      "           1       0.53      1.00      0.69       394\n",
      "\n",
      "    accuracy                           0.53       749\n",
      "   macro avg       0.26      0.50      0.34       749\n",
      "weighted avg       0.28      0.53      0.36       749\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Boosting Performance:\n",
      "Accuracy: 0.5808\n",
      "Confusion Matrix:\n",
      "[[252 103]\n",
      " [211 183]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.71      0.62       355\n",
      "           1       0.64      0.46      0.54       394\n",
      "\n",
      "    accuracy                           0.58       749\n",
      "   macro avg       0.59      0.59      0.58       749\n",
      "weighted avg       0.59      0.58      0.58       749\n",
      "\n",
      "\n",
      "K-Nearest Neighbors Performance:\n",
      "Accuracy: 0.6649\n",
      "Confusion Matrix:\n",
      "[[217 138]\n",
      " [113 281]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.61      0.63       355\n",
      "           1       0.67      0.71      0.69       394\n",
      "\n",
      "    accuracy                           0.66       749\n",
      "   macro avg       0.66      0.66      0.66       749\n",
      "weighted avg       0.66      0.66      0.66       749\n",
      "\n",
      "\n",
      "Decision Tree Performance:\n",
      "Accuracy: 0.5794\n",
      "Confusion Matrix:\n",
      "[[ 59 296]\n",
      " [ 19 375]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.17      0.27       355\n",
      "           1       0.56      0.95      0.70       394\n",
      "\n",
      "    accuracy                           0.58       749\n",
      "   macro avg       0.66      0.56      0.49       749\n",
      "weighted avg       0.65      0.58      0.50       749\n",
      "\n",
      "\n",
      "AdaBoost Performance:\n",
      "Accuracy: 0.5274\n",
      "Confusion Matrix:\n",
      "[[  2 353]\n",
      " [  1 393]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.01      0.01       355\n",
      "           1       0.53      1.00      0.69       394\n",
      "\n",
      "    accuracy                           0.53       749\n",
      "   macro avg       0.60      0.50      0.35       749\n",
      "weighted avg       0.59      0.53      0.37       749\n",
      "\n",
      "\n",
      "SGD Classifier Performance:\n",
      "Accuracy: 0.8438\n",
      "Confusion Matrix:\n",
      "[[265  90]\n",
      " [ 27 367]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.75      0.82       355\n",
      "           1       0.80      0.93      0.86       394\n",
      "\n",
      "    accuracy                           0.84       749\n",
      "   macro avg       0.86      0.84      0.84       749\n",
      "weighted avg       0.85      0.84      0.84       749\n",
      "\n",
      "\n",
      "MLP Performance:\n",
      "Accuracy: 0.8732\n",
      "Confusion Matrix:\n",
      "[[316  39]\n",
      " [ 56 338]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87       355\n",
      "           1       0.90      0.86      0.88       394\n",
      "\n",
      "    accuracy                           0.87       749\n",
      "   macro avg       0.87      0.87      0.87       749\n",
      "weighted avg       0.87      0.87      0.87       749\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 1098, number of negative: 647\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16830\n",
      "[LightGBM] [Info] Number of data points in the train set: 1745, number of used features: 66\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.629226 -> initscore=0.528899\n",
      "[LightGBM] [Info] Start training from score 0.528899\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "LGBMClassifier Performance:\n",
      "Accuracy: 0.6368\n",
      "Confusion Matrix:\n",
      "[[ 87 268]\n",
      " [  4 390]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.25      0.39       355\n",
      "           1       0.59      0.99      0.74       394\n",
      "\n",
      "    accuracy                           0.64       749\n",
      "   macro avg       0.77      0.62      0.57       749\n",
      "weighted avg       0.76      0.64      0.57       749\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Create list to store results\n",
    "results = []\n",
    "\n",
    "# Train-test loop\n",
    "for name, clf in best_models.items():\n",
    "    # Train model\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results.append((name, accuracy, cm))\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\n{name} Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Select top 10 features\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Performance:\n",
      "Accuracy: 0.6822\n",
      "Confusion Matrix:\n",
      "[[136 219]\n",
      " [ 19 375]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.38      0.53       355\n",
      "           1       0.63      0.95      0.76       394\n",
      "\n",
      "    accuracy                           0.68       749\n",
      "   macro avg       0.75      0.67      0.65       749\n",
      "weighted avg       0.75      0.68      0.65       749\n",
      "\n",
      "\n",
      "Random Forest Performance:\n",
      "Accuracy: 0.5554\n",
      "Confusion Matrix:\n",
      "[[ 42 313]\n",
      " [ 20 374]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.12      0.20       355\n",
      "           1       0.54      0.95      0.69       394\n",
      "\n",
      "    accuracy                           0.56       749\n",
      "   macro avg       0.61      0.53      0.45       749\n",
      "weighted avg       0.61      0.56      0.46       749\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Performance:\n",
      "Accuracy: 0.5260\n",
      "Confusion Matrix:\n",
      "[[  0 355]\n",
      " [  0 394]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       355\n",
      "           1       0.53      1.00      0.69       394\n",
      "\n",
      "    accuracy                           0.53       749\n",
      "   macro avg       0.26      0.50      0.34       749\n",
      "weighted avg       0.28      0.53      0.36       749\n",
      "\n",
      "\n",
      "SVM Performance:\n",
      "Accuracy: 0.6822\n",
      "Confusion Matrix:\n",
      "[[140 215]\n",
      " [ 23 371]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.39      0.54       355\n",
      "           1       0.63      0.94      0.76       394\n",
      "\n",
      "    accuracy                           0.68       749\n",
      "   macro avg       0.75      0.67      0.65       749\n",
      "weighted avg       0.74      0.68      0.65       749\n",
      "\n",
      "\n",
      "Naive Bayes Performance:\n",
      "Accuracy: 0.5260\n",
      "Confusion Matrix:\n",
      "[[  0 355]\n",
      " [  0 394]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       355\n",
      "           1       0.53      1.00      0.69       394\n",
      "\n",
      "    accuracy                           0.53       749\n",
      "   macro avg       0.26      0.50      0.34       749\n",
      "weighted avg       0.28      0.53      0.36       749\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Boosting Performance:\n",
      "Accuracy: 0.5768\n",
      "Confusion Matrix:\n",
      "[[ 57 298]\n",
      " [ 19 375]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.16      0.26       355\n",
      "           1       0.56      0.95      0.70       394\n",
      "\n",
      "    accuracy                           0.58       749\n",
      "   macro avg       0.65      0.56      0.48       749\n",
      "weighted avg       0.65      0.58      0.50       749\n",
      "\n",
      "\n",
      "K-Nearest Neighbors Performance:\n",
      "Accuracy: 0.5567\n",
      "Confusion Matrix:\n",
      "[[ 56 299]\n",
      " [ 33 361]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.16      0.25       355\n",
      "           1       0.55      0.92      0.69       394\n",
      "\n",
      "    accuracy                           0.56       749\n",
      "   macro avg       0.59      0.54      0.47       749\n",
      "weighted avg       0.59      0.56      0.48       749\n",
      "\n",
      "\n",
      "Decision Tree Performance:\n",
      "Accuracy: 0.5314\n",
      "Confusion Matrix:\n",
      "[[  4 351]\n",
      " [  0 394]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.02       355\n",
      "           1       0.53      1.00      0.69       394\n",
      "\n",
      "    accuracy                           0.53       749\n",
      "   macro avg       0.76      0.51      0.36       749\n",
      "weighted avg       0.75      0.53      0.37       749\n",
      "\n",
      "\n",
      "AdaBoost Performance:\n",
      "Accuracy: 0.5274\n",
      "Confusion Matrix:\n",
      "[[  2 353]\n",
      " [  1 393]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.01      0.01       355\n",
      "           1       0.53      1.00      0.69       394\n",
      "\n",
      "    accuracy                           0.53       749\n",
      "   macro avg       0.60      0.50      0.35       749\n",
      "weighted avg       0.59      0.53      0.37       749\n",
      "\n",
      "\n",
      "SGD Classifier Performance:\n",
      "Accuracy: 0.5287\n",
      "Confusion Matrix:\n",
      "[[  3 352]\n",
      " [  1 393]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.01      0.02       355\n",
      "           1       0.53      1.00      0.69       394\n",
      "\n",
      "    accuracy                           0.53       749\n",
      "   macro avg       0.64      0.50      0.35       749\n",
      "weighted avg       0.63      0.53      0.37       749\n",
      "\n",
      "\n",
      "MLP Performance:\n",
      "Accuracy: 0.6128\n",
      "Confusion Matrix:\n",
      "[[ 72 283]\n",
      " [  7 387]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.20      0.33       355\n",
      "           1       0.58      0.98      0.73       394\n",
      "\n",
      "    accuracy                           0.61       749\n",
      "   macro avg       0.74      0.59      0.53       749\n",
      "weighted avg       0.74      0.61      0.54       749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Updated classifier dictionary\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(C=10, max_iter=1000, solver='liblinear'),\n",
    "    'Random Forest': RandomForestClassifier(max_depth=10, min_samples_split=10, n_estimators=50),\n",
    "    'XGBoost': XGBClassifier(learning_rate=0.01, max_depth=3, n_estimators=100, eval_metric='logloss'),\n",
    "    'SVM': SVC(C=10, kernel='linear', probability=True),\n",
    "    'Naive Bayes': BernoulliNB(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3),  # Uses decision trees [[7]]\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),  # Instance-based learning\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, min_samples_split=10),  # Base tree model [[7]]\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=50, learning_rate=0.5),  # Boosting ensemble\n",
    "    'SGD Classifier': SGDClassifier(loss='log_loss', alpha=0.001),  # Linear model with SGD [[10]]\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)  # Neural network\n",
    "}\n",
    "# Create list to store results\n",
    "results = []\n",
    "\n",
    "# Train-test loop\n",
    "for name, clf in classifiers.items():\n",
    "    # Train model\n",
    "    clf.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test_selected)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results.append((name, accuracy, cm))\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\n{name} Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: imblearn in /home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages (from imblearn) (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (2.1.3)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Performance:\n",
      "Accuracy: 0.8838\n",
      "Confusion Matrix:\n",
      "[[316  39]\n",
      " [ 48 346]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88       355\n",
      "           1       0.90      0.88      0.89       394\n",
      "\n",
      "    accuracy                           0.88       749\n",
      "   macro avg       0.88      0.88      0.88       749\n",
      "weighted avg       0.88      0.88      0.88       749\n",
      "\n",
      "\n",
      "Random Forest Performance:\n",
      "Accuracy: 0.6449\n",
      "Confusion Matrix:\n",
      "[[113 242]\n",
      " [ 24 370]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.32      0.46       355\n",
      "           1       0.60      0.94      0.74       394\n",
      "\n",
      "    accuracy                           0.64       749\n",
      "   macro avg       0.71      0.63      0.60       749\n",
      "weighted avg       0.71      0.64      0.60       749\n",
      "\n",
      "\n",
      "XGBoost Performance:\n",
      "Accuracy: 0.6061\n",
      "Confusion Matrix:\n",
      "[[ 81 274]\n",
      " [ 21 373]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.23      0.35       355\n",
      "           1       0.58      0.95      0.72       394\n",
      "\n",
      "    accuracy                           0.61       749\n",
      "   macro avg       0.69      0.59      0.54       749\n",
      "weighted avg       0.68      0.61      0.54       749\n",
      "\n",
      "\n",
      "SVM Performance:\n",
      "Accuracy: 0.8865\n",
      "Confusion Matrix:\n",
      "[[324  31]\n",
      " [ 54 340]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.91      0.88       355\n",
      "           1       0.92      0.86      0.89       394\n",
      "\n",
      "    accuracy                           0.89       749\n",
      "   macro avg       0.89      0.89      0.89       749\n",
      "weighted avg       0.89      0.89      0.89       749\n",
      "\n",
      "\n",
      "Naive Bayes Performance:\n",
      "Accuracy: 0.5260\n",
      "Confusion Matrix:\n",
      "[[  0 355]\n",
      " [  0 394]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       355\n",
      "           1       0.53      1.00      0.69       394\n",
      "\n",
      "    accuracy                           0.53       749\n",
      "   macro avg       0.26      0.50      0.34       749\n",
      "weighted avg       0.28      0.53      0.36       749\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Define classifiers with corrected hyperparameters\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(C=10, max_iter=1000, solver='liblinear'),\n",
    "    'Random Forest': RandomForestClassifier(max_depth=10, min_samples_split=10, n_estimators=50),\n",
    "    'XGBoost': XGBClassifier(learning_rate=0.01, max_depth=3, n_estimators=100, eval_metric='logloss'),\n",
    "    'SVM': SVC(C=10, kernel='linear', probability=True),\n",
    "    'Naive Bayes': BernoulliNB()\n",
    "}\n",
    "\n",
    "# Create list to store results\n",
    "results = []\n",
    "\n",
    "# Train-test loop\n",
    "for name, clf in classifiers.items():\n",
    "    # Train model\n",
    "    clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results.append((name, accuracy, cm))\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\n{name} Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;Random Forest&#x27;,\n",
       "                              RandomForestClassifier(max_depth=10,\n",
       "                                                     n_estimators=50)),\n",
       "                             (&#x27;Logistic Regression&#x27;,\n",
       "                              LogisticRegression(C=10, max_iter=1000,\n",
       "                                                 solver=&#x27;liblinear&#x27;)),\n",
       "                             (&#x27;SVM&#x27;,\n",
       "                              SVC(C=10, kernel=&#x27;linear&#x27;, probability=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>VotingClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.VotingClassifier.html\">?<span>Documentation for VotingClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>VotingClassifier(estimators=[(&#x27;Random Forest&#x27;,\n",
       "                              RandomForestClassifier(max_depth=10,\n",
       "                                                     n_estimators=50)),\n",
       "                             (&#x27;Logistic Regression&#x27;,\n",
       "                              LogisticRegression(C=10, max_iter=1000,\n",
       "                                                 solver=&#x27;liblinear&#x27;)),\n",
       "                             (&#x27;SVM&#x27;,\n",
       "                              SVC(C=10, kernel=&#x27;linear&#x27;, probability=True))])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>Random Forest</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(max_depth=10, n_estimators=50)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>Logistic Regression</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=10, max_iter=1000, solver=&#x27;liblinear&#x27;)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><label>SVM</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SVC</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(C=10, kernel=&#x27;linear&#x27;, probability=True)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('Random Forest',\n",
       "                              RandomForestClassifier(max_depth=10,\n",
       "                                                     n_estimators=50)),\n",
       "                             ('Logistic Regression',\n",
       "                              LogisticRegression(C=10, max_iter=1000,\n",
       "                                                 solver='liblinear')),\n",
       "                             ('SVM',\n",
       "                              SVC(C=10, kernel='linear', probability=True))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('Random Forest', best_models['Random Forest']),\n",
    "    ('Logistic Regression', best_models['Logistic Regression']),\n",
    "    ('SVM', best_models['SVM'])\n",
    "], voting='hard')  # 'soft' for probability averaging\n",
    "\n",
    "voting_clf.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = voting_clf.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Performance:\n",
      "Accuracy: 0.8892\n",
      "Confusion Matrix:\n",
      "[[317  38]\n",
      " [ 45 349]]\n"
     ]
    }
   ],
   "source": [
    "print(f\" Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression (Optimized) Performance:\n",
      "Accuracy: 0.8879\n",
      "Confusion Matrix:\n",
      "[[310  45]\n",
      " [ 39 355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88       355\n",
      "           1       0.89      0.90      0.89       394\n",
      "\n",
      "    accuracy                           0.89       749\n",
      "   macro avg       0.89      0.89      0.89       749\n",
      "weighted avg       0.89      0.89      0.89       749\n",
      "\n",
      "\n",
      "Random Forest (Optimized) Performance:\n",
      "Accuracy: 0.6088\n",
      "Confusion Matrix:\n",
      "[[255 100]\n",
      " [193 201]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.72      0.64       355\n",
      "           1       0.67      0.51      0.58       394\n",
      "\n",
      "    accuracy                           0.61       749\n",
      "   macro avg       0.62      0.61      0.61       749\n",
      "weighted avg       0.62      0.61      0.61       749\n",
      "\n",
      "\n",
      "XGBoost (Optimized) Performance:\n",
      "Accuracy: 0.5394\n",
      "Confusion Matrix:\n",
      "[[ 10 345]\n",
      " [  0 394]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.03      0.05       355\n",
      "           1       0.53      1.00      0.70       394\n",
      "\n",
      "    accuracy                           0.54       749\n",
      "   macro avg       0.77      0.51      0.38       749\n",
      "weighted avg       0.75      0.54      0.39       749\n",
      "\n",
      "\n",
      "SVM (Optimized) Performance:\n",
      "Accuracy: 0.8865\n",
      "Confusion Matrix:\n",
      "[[319  36]\n",
      " [ 49 345]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.90      0.88       355\n",
      "           1       0.91      0.88      0.89       394\n",
      "\n",
      "    accuracy                           0.89       749\n",
      "   macro avg       0.89      0.89      0.89       749\n",
      "weighted avg       0.89      0.89      0.89       749\n",
      "\n",
      "\n",
      "Naive Bayes (Optimized) Performance:\n",
      "Accuracy: 0.5260\n",
      "Confusion Matrix:\n",
      "[[  0 355]\n",
      " [  0 394]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       355\n",
      "           1       0.53      1.00      0.69       394\n",
      "\n",
      "    accuracy                           0.53       749\n",
      "   macro avg       0.26      0.50      0.34       749\n",
      "weighted avg       0.28      0.53      0.36       749\n",
      "\n",
      "\n",
      "Gradient Boosting (Optimized) Performance:\n",
      "Accuracy: 0.5808\n",
      "Confusion Matrix:\n",
      "[[252 103]\n",
      " [211 183]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.71      0.62       355\n",
      "           1       0.64      0.46      0.54       394\n",
      "\n",
      "    accuracy                           0.58       749\n",
      "   macro avg       0.59      0.59      0.58       749\n",
      "weighted avg       0.59      0.58      0.58       749\n",
      "\n",
      "\n",
      "K-Nearest Neighbors (Optimized) Performance:\n",
      "Accuracy: 0.6649\n",
      "Confusion Matrix:\n",
      "[[217 138]\n",
      " [113 281]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.61      0.63       355\n",
      "           1       0.67      0.71      0.69       394\n",
      "\n",
      "    accuracy                           0.66       749\n",
      "   macro avg       0.66      0.66      0.66       749\n",
      "weighted avg       0.66      0.66      0.66       749\n",
      "\n",
      "\n",
      "Decision Tree (Optimized) Performance:\n",
      "Accuracy: 0.5794\n",
      "Confusion Matrix:\n",
      "[[ 59 296]\n",
      " [ 19 375]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.17      0.27       355\n",
      "           1       0.56      0.95      0.70       394\n",
      "\n",
      "    accuracy                           0.58       749\n",
      "   macro avg       0.66      0.56      0.49       749\n",
      "weighted avg       0.65      0.58      0.50       749\n",
      "\n",
      "\n",
      "AdaBoost (Optimized) Performance:\n",
      "Accuracy: 0.5274\n",
      "Confusion Matrix:\n",
      "[[  2 353]\n",
      " [  1 393]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.01      0.01       355\n",
      "           1       0.53      1.00      0.69       394\n",
      "\n",
      "    accuracy                           0.53       749\n",
      "   macro avg       0.60      0.50      0.35       749\n",
      "weighted avg       0.59      0.53      0.37       749\n",
      "\n",
      "\n",
      "SGD Classifier (Optimized) Performance:\n",
      "Accuracy: 0.8438\n",
      "Confusion Matrix:\n",
      "[[265  90]\n",
      " [ 27 367]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.75      0.82       355\n",
      "           1       0.80      0.93      0.86       394\n",
      "\n",
      "    accuracy                           0.84       749\n",
      "   macro avg       0.86      0.84      0.84       749\n",
      "weighted avg       0.85      0.84      0.84       749\n",
      "\n",
      "\n",
      "MLP (Optimized) Performance:\n",
      "Accuracy: 0.8732\n",
      "Confusion Matrix:\n",
      "[[316  39]\n",
      " [ 56 338]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87       355\n",
      "           1       0.90      0.86      0.88       394\n",
      "\n",
      "    accuracy                           0.87       749\n",
      "   macro avg       0.87      0.87      0.87       749\n",
      "weighted avg       0.87      0.87      0.87       749\n",
      "\n",
      "\n",
      "LGBMClassifier (Optimized) Performance:\n",
      "Accuracy: 0.6368\n",
      "Confusion Matrix:\n",
      "[[ 87 268]\n",
      " [  4 390]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.25      0.39       355\n",
      "           1       0.59      0.99      0.74       394\n",
      "\n",
      "    accuracy                           0.64       749\n",
      "   macro avg       0.77      0.62      0.57       749\n",
      "weighted avg       0.76      0.64      0.57       749\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/sacsresta/miniconda3/envs/research/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\n{name} (Optimized) Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 21:26:48.345444: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743071208.418259  227806 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743071208.439288  227806 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743071208.607224  227806 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743071208.607249  227806 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743071208.607252  227806 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743071208.607253  227806 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-27 21:26:48.627022: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 21:26:50.920343: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2025-03-27 21:26:50.920408: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n",
      "2025-03-27 21:26:50.920414: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: Sachin\n",
      "2025-03-27 21:26:50.920417: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: Sachin\n",
      "2025-03-27 21:26:50.920519: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 550.120.0\n",
      "2025-03-27 21:26:50.920533: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 550.120.0\n",
      "2025-03-27 21:26:50.920535: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 550.120.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6081 - loss: 0.6695 - val_accuracy: 0.6476 - val_loss: 0.5836\n",
      "Epoch 2/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7144 - loss: 0.5777 - val_accuracy: 0.3553 - val_loss: 1.2483\n",
      "Epoch 3/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7236 - loss: 0.5533 - val_accuracy: 0.7794 - val_loss: 0.5255\n",
      "Epoch 4/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7659 - loss: 0.4909 - val_accuracy: 0.5330 - val_loss: 0.8610\n",
      "Epoch 5/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7554 - loss: 0.4823 - val_accuracy: 0.5244 - val_loss: 0.9973\n",
      "Epoch 6/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7777 - loss: 0.4790 - val_accuracy: 0.7135 - val_loss: 0.4598\n",
      "Epoch 7/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7649 - loss: 0.4812 - val_accuracy: 0.3610 - val_loss: 1.8828\n",
      "Epoch 8/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7981 - loss: 0.4300 - val_accuracy: 0.4756 - val_loss: 0.9993\n",
      "Epoch 9/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8146 - loss: 0.4151 - val_accuracy: 0.4040 - val_loss: 1.5853\n",
      "Epoch 10/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8157 - loss: 0.3847 - val_accuracy: 0.4728 - val_loss: 1.3127\n",
      "Epoch 11/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8050 - loss: 0.4018 - val_accuracy: 0.7679 - val_loss: 0.5388\n",
      "Epoch 12/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8131 - loss: 0.3848 - val_accuracy: 0.4585 - val_loss: 1.3839\n",
      "Epoch 13/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8272 - loss: 0.3770 - val_accuracy: 0.5415 - val_loss: 0.9578\n",
      "Epoch 14/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8129 - loss: 0.3902 - val_accuracy: 0.5559 - val_loss: 0.9425\n",
      "Epoch 15/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8610 - loss: 0.3467 - val_accuracy: 0.5272 - val_loss: 1.0881\n",
      "Epoch 16/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8751 - loss: 0.3281 - val_accuracy: 0.7851 - val_loss: 0.4866\n",
      "Epoch 17/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8521 - loss: 0.3213 - val_accuracy: 0.5530 - val_loss: 1.4041\n",
      "Epoch 18/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8374 - loss: 0.3499 - val_accuracy: 0.8596 - val_loss: 0.3114\n",
      "Epoch 19/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8577 - loss: 0.3359 - val_accuracy: 0.8596 - val_loss: 0.3381\n",
      "Epoch 20/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8545 - loss: 0.3143 - val_accuracy: 0.8023 - val_loss: 0.4832\n",
      "Epoch 21/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8789 - loss: 0.3001 - val_accuracy: 0.7335 - val_loss: 0.6129\n",
      "Epoch 22/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8558 - loss: 0.3197 - val_accuracy: 0.7564 - val_loss: 0.5827\n",
      "Epoch 23/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8657 - loss: 0.2984 - val_accuracy: 0.4499 - val_loss: 2.0073\n",
      "Epoch 24/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8571 - loss: 0.3060 - val_accuracy: 0.5845 - val_loss: 0.9955\n",
      "Epoch 25/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8656 - loss: 0.2889 - val_accuracy: 0.8653 - val_loss: 0.3180\n",
      "Epoch 26/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8767 - loss: 0.2959 - val_accuracy: 0.6189 - val_loss: 1.0909\n",
      "Epoch 27/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8987 - loss: 0.2630 - val_accuracy: 0.5616 - val_loss: 1.2113\n",
      "Epoch 28/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8554 - loss: 0.2978 - val_accuracy: 0.8453 - val_loss: 0.3767\n",
      "Epoch 29/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8770 - loss: 0.2846 - val_accuracy: 0.8281 - val_loss: 0.4148\n",
      "Epoch 30/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8737 - loss: 0.2965 - val_accuracy: 0.8567 - val_loss: 0.3152\n",
      "Epoch 31/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8807 - loss: 0.2865 - val_accuracy: 0.7135 - val_loss: 0.6662\n",
      "Epoch 32/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8685 - loss: 0.2798 - val_accuracy: 0.5530 - val_loss: 1.1175\n",
      "Epoch 33/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8728 - loss: 0.2749 - val_accuracy: 0.6734 - val_loss: 0.8095\n",
      "Epoch 34/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8778 - loss: 0.2708 - val_accuracy: 0.8453 - val_loss: 0.3866\n",
      "Epoch 35/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8700 - loss: 0.2877 - val_accuracy: 0.6218 - val_loss: 1.0021\n",
      "Epoch 36/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8773 - loss: 0.2859 - val_accuracy: 0.6189 - val_loss: 1.0504\n",
      "Epoch 37/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8784 - loss: 0.2566 - val_accuracy: 0.8138 - val_loss: 0.4605\n",
      "Epoch 38/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8869 - loss: 0.2748 - val_accuracy: 0.6017 - val_loss: 1.0719\n",
      "Epoch 39/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8796 - loss: 0.2657 - val_accuracy: 0.4097 - val_loss: 2.2018\n",
      "Epoch 40/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8828 - loss: 0.2575 - val_accuracy: 0.7106 - val_loss: 0.6897\n",
      "Epoch 41/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8471 - loss: 0.3358 - val_accuracy: 0.3811 - val_loss: 2.2958\n",
      "Epoch 42/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8838 - loss: 0.2907 - val_accuracy: 0.7393 - val_loss: 0.6112\n",
      "Epoch 43/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8777 - loss: 0.2841 - val_accuracy: 0.5817 - val_loss: 1.0780\n",
      "Epoch 44/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8882 - loss: 0.2672 - val_accuracy: 0.5788 - val_loss: 0.9905\n",
      "Epoch 45/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8796 - loss: 0.2706 - val_accuracy: 0.5358 - val_loss: 1.4302\n",
      "Epoch 46/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8801 - loss: 0.2737 - val_accuracy: 0.6218 - val_loss: 0.9778\n",
      "Epoch 47/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8690 - loss: 0.2970 - val_accuracy: 0.5559 - val_loss: 1.1802\n",
      "Epoch 48/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8811 - loss: 0.2671 - val_accuracy: 0.5158 - val_loss: 1.4739\n",
      "Epoch 49/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8783 - loss: 0.2762 - val_accuracy: 0.5415 - val_loss: 1.2371\n",
      "Epoch 50/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8866 - loss: 0.2340 - val_accuracy: 0.5072 - val_loss: 1.6406\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),  # Dropout to reduce overfitting\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')  # Multi-class: softmax, Binary: sigmoid\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, \n",
    "                    epochs=50, batch_size=32, \n",
    "                    validation_split=0.2, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 996us/step\n"
     ]
    }
   ],
   "source": [
    "# First get raw probabilities for precision threshold tuning\n",
    "y_probs = model.predict(X_test_scaled)\n",
    "\n",
    "# Apply threshold optimized via validation data (replace with your method from [[2]] [[7]])\n",
    "optimal_threshold = 0.63  # Example from validation analysis in search result [[2]]\n",
    "y_pred = y_probs > optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6368\n",
      "Confusion Matrix:\n",
      "[[ 87 268]\n",
      " [  4 390]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      1.00      0.66       355\n",
      "           1       1.00      0.06      0.12       394\n",
      "\n",
      "    accuracy                           0.51       749\n",
      "   macro avg       0.75      0.53      0.39       749\n",
      "weighted avg       0.76      0.51      0.37       749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled shape: (1745, 66)\n",
      "y_train_encoded shape: (1745,)\n",
      "Missing values in X_train_scaled: 0\n",
      "Missing values in y_train_encoded: N/A\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"y_train_encoded shape:\", y_train_encoded.shape)\n",
    "\n",
    "print(\"Missing values in X_train_scaled:\", np.isnan(X_train_scaled).sum())\n",
    "print(\"Missing values in y_train_encoded:\", np.isnan(y_train_encoded).sum() if num_classes > 2 else \"N/A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1745, 66)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
